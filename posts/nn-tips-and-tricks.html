<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Daniel Waterworth">
  <meta name="dcterms.date" content="2017-09-30">
  <title>NNs: Tips and Tricks</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="./../css/main.css">
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
</head>
<body>
<header>
<h1 class="title">NNs: Tips and Tricks</h1>
<h2 class="author">Daniel Waterworth</h2>
<h3 class="date">September 30, 2017</h3>
</header>
<h2 id="background">Background</h2>
<p>I been interested in machine learning since before it was cool, but it’s only relatively recently that I’ve really begun training my own neural networks. So, whilst it’s all fresh in my mind, I want to jot down some notes that may be helpful for my future self and others. I intend to update this document in the future as I discover new things.</p>
<h2 id="padding">1. Padding</h2>
<p>In my experience, padding causes more problems than it solves. When you pad before convolving, you are forcing your convolutions to handle two different cases with the same weights; the case where the convolution window is somewhere in the centre and the case where it is hanging over an edge. You can mitigate this somewhat by adding a channel of 1s before padding, but with a little thought, its possible to redesign your neural network so that it doesn’t pad at all and the results tend to be much better.</p>
<h2 id="transposed-convolutions">2. Transposed Convolutions</h2>
<p>Again, avoid. Transposed convolutions tend to produce blocking artefacts. This effect is well documented <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. When you need to scale up, use nearest neighbor or bilinear upsampling.</p>
<h2 id="strided-convolutions">3. Strided Convolutions</h2>
<p>Think of a strided convolution as the combination of a dense convolution and an operation that throws away alternate rows and columns. The information in all those rows and columns may have been useful. My advice is: allow the neural network to decide what information to keep. When you need to scale down spatially, do a space-to-depth conversion and potentially follow that with a 1x1 convolution to reduce the number of channels.</p>
<section class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="https://distill.pub/2016/deconv-checkerboard/">Deconvolution and Checkerboard Artifacts</a><a href="#fnref1">↩</a></p></li>
</ol>
</section>
</body>
</html>
